---

- name: Compute node initialisation
  hosts: localhost
  become: yes
  vars:
    control_node_ip: "172.16.1.228"
    nfs_export_hosts: "/exports/hosts"
    resolv_conf_nameservers: [1.1.1.1, 8.8.8.8]

    # block device (disk) on which to create the exported filesystem.
    # if the disk is not defined, formatting and mounting will not be done.
    nfs_disk_location:
    # Path to exported filesystem mountpoint on nfs servers
    nfs_export: "/exports/home"
    # nfs client mount options
    nfs_client_mnt_options:
    # Path to mountpoint on nfs clients
    nfs_client_mnt_point: "/home"
    nfs_client_mnt_state: mounted
    nfs_server: "{{ control_node_ip }}"


    os_manila_mount_shares: []
    os_manila_mount_state: mounted
    os_manila_mount_opts:
      - x-systemd.device-timeout=30
      - x-systemd.mount-timeout=30
      - noatime
      - _netdev # prevents mount blocking early boot before networking available
      - rw
    os_manila_mount_share_info: [] # populated by lookup mode
    os_manila_mount_ceph_conf_path: /etc/ceph


    basic_users_manage_homedir: false
    basic_users_userdefaults:
      state: present
      create_home: "{{ basic_users_manage_homedir }}"
      generate_ssh_key: "{{ basic_users_manage_homedir }}"
      ssh_key_comment: "{{ item.name }}"
    test_user_password: "zXpcWyGQL7jtZnqylQra4g=="
    basic_users_users:
      - name: testuser # can't use rocky as $HOME isn't shared!
        password: "{{ test_user_password | password_hash('sha512', 65534 | random(seed=inventory_hostname) | string) }}" # idempotent
        uid: 1005
        state: present
    basic_users_groups: []


    # Default to 10GB
    cvmfs_quota_limit_mb: 10000
    cvmfs_config_default:
      CVMFS_CLIENT_PROFILE: single
      CVMFS_QUOTA_LIMIT: "{{ cvmfs_quota_limit_mb }}"
    cvmfs_config_overrides: {}
    cvmfs_config: "{{ cvmfs_config_default | combine(cvmfs_config_overrides) }}"


  tasks:
    - name: Configure resolve.conf
      block:
        - name: Set nameservers in /etc/resolv.conf
          ansible.builtin.template:
            src: /etc/ansible-init/templates/resolv.conf.j2
            dest: /etc/resolv.conf
            owner: root
            group: root
            mode: u=rw,og=r

        - name: Disable NetworkManager control of resolv.conf
          ansible.builtin.copy:
            src: /etc/ansible-init/files/NetworkManager-dns-none.conf
            dest: /etc/NetworkManager/conf.d/90-dns-none.conf
            owner: root
            group: root
            mode: u=rw,og=r
          register: _copy_nm_config

        - name: Reload NetworkManager
          ansible.builtin.systemd:
            name: NetworkManager
            state: reloaded
          when: _copy_nm_config.changed | default(false)


    - name: Mount /etc/hosts on compute nodes
      block:
        - name: Ensure the mount directory exists
          file:
            path: /mnt/hosts
            state: directory
            mode: 0755
        
        - name: Mount /mnt/hosts
          mount:
            path: /mnt/hosts
            src: "{{ vars.control_node_ip }}:{{ nfs_export_hosts }}"
            fstype: nfs
            opts: rw,sync
            state: mounted

        - name: Copy /mnt/hosts/hosts contents to /etc/hosts
          copy:
            src: /mnt/hosts/hosts
            dest: /etc/hosts
            owner: root
            group: root
            mode: 0644


    - name: NFS client mount
      block:
        - name: ensure mount directory exists
          file:
            path: "{{ nfs_client_mnt_point }}"
            state: directory

        - name: mount the filesystem
          mount:
            path: "{{ nfs_client_mnt_point }}"
            src: "{{ nfs_server }}:{{ nfs_export }}"
            fstype: nfs
            state: "{{ nfs_client_mnt_state }}"


    - name: Manila mount
      block:
        - name: Read manila share from nfs file
          slurp:
            src: "/mnt/cluster/manila_share_info.yml"
          register: manila_share_info_file

        - name: Parse and set fact for manila share info
          set_fact:
            os_manila_mount_share_info: "{{ manila_share_info_file.content | b64decode | from_yaml }}"

        - name: Ensure Ceph configuration directory exists
          ansible.builtin.file:
            path: "{{ os_manila_mount_ceph_conf_path }}"
            state: directory
            mode: "0755"
            owner: root
            group: root

        - name: Configure ceph.conf using os_manila_mount_host
          ansible.builtin.template:
            src: /etc/ansible-init/templates/ceph.conf.j2
            dest: "{{ os_manila_mount_ceph_conf_path }}/ceph.conf"
            owner: root
            group: root
            mode: "0600"

        - name: Ensure mount directory exists
          ansible.builtin.file:
            path: "{{ item.mount_path }}"
            state: directory
            owner: "{{ item.mount_user | default(omit) }}"
            group: "{{ item.mount_group | default(omit) }}"
            mode: "{{ item.mount_mode | default(omit) }}"
          loop: "{{ os_manila_mount_shares }}"
          loop_control:
            label: "{{ item.share_name }}"

        - name: Write Ceph client keyring
          ansible.builtin.template:
            src: /etc/ansible-init/templates/ceph.keyring.j2
            dest: "{{ os_manila_mount_ceph_conf_path }}/ceph.client.{{ item.share_user }}.keyring"
            mode: "0600"
            owner: root
            group: root
          loop: "{{ os_manila_mount_share_info }}"
          loop_control:
            label: "{{ item.share_name }}"

        - name: Mount the Ceph share
          ansible.posix.mount:
            path: "{{ item[0].mount_path }}"
            src: "{{ item[1].host }}:{{ item[1].export }}"
            fstype: ceph
            opts: "name={{ item[1].share_user }},{{ (item[0].mount_opts | default(os_manila_mount_opts)) | join(',') }}"
            # NB share_user is looked up here in case of autodetection
            state: "{{ item[0].mount_state | default(os_manila_mount_state) }}"
          loop: "{{ os_manila_mount_shares | zip(os_manila_mount_share_info) }}"
          loop_control:
            label: "{{ item[0].share_name }}"

        - name: Ensure mounted directory has correct permissions
          ansible.builtin.file:
            path: "{{ item.mount_path }}"
            state: directory
            owner: "{{ item.mount_user | default(omit) }}"
            group: "{{ item.mount_group | default(omit) }}"
            mode: "{{ item.mount_mode | default(omit) }}"
          loop: "{{ os_manila_mount_shares }}"
          loop_control:
            label: "{{ item.share_name }}"
          when: item.mount_state | default(os_manila_mount_state) in ['mounted' or 'ephemeral']


    - name: Basic users setup
      block:
        - name: Create groups
          ansible.builtin.group: "{{ item }}"
          loop:  "{{ basic_users_groups }}"

        - name: Create users
          user: "{{ basic_users_userdefaults | combine(item) | filter_user_params() }}"
          loop: "{{ basic_users_users }}"
          loop_control:
            label: "{{ item.name }} [{{ item.state | default('present') }}]"
          register: basic_users_info

        - name: Write sudo rules
          blockinfile:
            path: /etc/sudoers.d/80-{{ item.name}}-user
            block: "{{ item.sudo }}"
            create: true
          loop: "{{ basic_users_users }}"
          loop_control:
            label: "{{ item.name }}"
          when: "'sudo' in item"


    - name: Configure EESSI
      gather_facts: false
      block:
        - name: Download Cern GPG key
          ansible.builtin.get_url:
            url: http://cvmrepo.web.cern.ch/cvmrepo/yum/RPM-GPG-KEY-CernVM 
            dest: ./cvmfs-key.gpg

        - name: Import downloaded GPG key
          command: rpm --import cvmfs-key.gpg

        - name: Add CVMFS repo
          dnf:
            name: https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm

        - name: Install CVMFS
          dnf:
            name: cvmfs

        - name: Install EESSI CVMFS config
          dnf:
            name: https://github.com/EESSI/filesystem-layer/releases/download/latest/cvmfs-config-eessi-latest.noarch.rpm
            # NOTE: Can't find any docs on obtaining gpg key - maybe downloading directly from github is ok?
            disable_gpg_check: true

        # Alternative version using official repo - still no GPG key :(
        # - name: Add EESSI repo
        #   dnf: 
        #     name: http://repo.eessi-infra.org/eessi/rhel/8/noarch/eessi-release-0-1.noarch.rpm

        # - name: Install EESSI CVMFS config
        #   dnf:
        #     name: cvmfs-config-eessi

        - name: Add base CVMFS config
          community.general.ini_file:
            dest: /etc/cvmfs/default.local
            section: null
            option: "{{ item.key }}"
            value: "{{ item.value }}"
            no_extra_spaces: true
          loop: "{{ cvmfs_config | dict2items }}"

        # NOTE: Not clear how to make this idempotent
        - name: Ensure CVMFS config is setup
          command:
            cmd: "cvmfs_config setup"


    - name: Configure openhpc
      block:
        - name: Check openhpc_slurm_control_host, openhpc_cluster_name or openhpc_slurm_partitions exist
          assert:
            that:
              - openhpc_slurm_control_host is defined
              - openhpc_cluster_name is defined
              - openhpc_cluster_name != ''
              - openhpc_slurm_partitions is defined
            fail_msg: "Undefined openhpc_slurm_control_host, openhpc_cluster_name or openhpc_slurm_partitions."

        - name: Fail if control host not in play and munge key not specified
          fail:
            msg: "Either the slurm control node must be in the play or `openhpc_munge_key` must be set"
          when:
            - openhpc_slurm_control_host not in ansible_play_hosts
            - not openhpc_munge_key

        # - name: Ensure Slurm directories exists
        #   file:
        #     path: "{{ openhpc_state_save_location }}"
        #     owner: slurm
        #     group: slurm
        #     mode: 0755
        #     state: directory
        #   when: inventory_hostname == openhpc_slurm_control_host

        # - name: Generate a Munge key on control host
        #   # NB this is usually a no-op as the package install actually generates a (node-unique) one, so won't usually trigger handler
        #   command: "dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024"
        #   args:
        #     creates: "/etc/munge/munge.key"
        #   when: inventory_hostname == openhpc_slurm_control_host

        # - name: Retrieve Munge key from control host
        #   slurp:
        #     src: "/etc/munge/munge.key"
        #   register: openhpc_control_munge_key
        #   delegate_to: "{{ openhpc_slurm_control_host }}"
        #   when: openhpc_slurm_control_host in ansible_play_hosts

        - name: Fix permissions on /etc to pass Munge startup checks
          # Rocky-9-GenericCloud-Base-9.4-20240523.0.x86_64.qcow2 makes /etc g=rwx rather than g=rx (where group=root)
          # which fails munged startup checks
          file:
            path: /etc
            state: directory
            mode: g-w

        - name: Write Munge key
          copy:
            content: "{{ openhpc_munge_key or (openhpc_control_munge_key.content | b64decode) }}"
            dest: "/etc/munge/munge.key"
            owner: munge
            group: munge
            mode: 0400
          notify:
            - Restart Munge service

        - name: Ensure JobComp logfile exists
          file:
            path: "{{ openhpc_slurm_job_comp_loc }}"
            state: touch
            owner: slurm
            group: slurm
            mode: 0644
            access_time: preserve
            modification_time: preserve
          when: openhpc_slurm_job_comp_type == 'jobcomp/filetxt'

        - name: Template slurmdbd.conf
          template:
            src: slurmdbd.conf.j2
            dest: /etc/slurm/slurmdbd.conf
            mode: "0600"
            owner: slurm
            group: slurm
          notify: Restart slurmdbd service
          when: openhpc_enable.database | default(false) | bool

        - name: Make local tempfile for slurm.conf templating # ensures simultaneous runs don't clobber each other
          ansible.builtin.tempfile:
          register: _slurm_conf_tmpfile
          delegate_to: localhost
          when: openhpc_enable.control | default(false) or not openhpc_slurm_configless
          changed_when: false # so molecule doesn't fail
          become: no

        - name: Template basic slurm.conf
          template:
            src: slurm.conf.j2
            dest: "{{ _slurm_conf_tmpfile.path }}"
            lstrip_blocks: true
            mode: 0644
          delegate_to: localhost
          when: openhpc_enable.control | default(false) or not openhpc_slurm_configless
          changed_when: false # so molecule doesn't fail
          become: no

        - name: Customise slurm.conf
          community.general.ini_file:
            path: "{{ _slurm_conf_tmpfile.path }}"
            option: "{{ item.key }}"
            section: ''
            value: "{{ (item.value | join(',')) if (item.value is sequence and item.value is not string) else item.value }}"
            no_extra_spaces: true
            create: no
            mode: 0644
          loop: "{{ openhpc_config | dict2items }}"
          delegate_to: localhost
          when: openhpc_enable.control | default(false) or not openhpc_slurm_configless
          changed_when: false # so molecule doesn't fail
          become: no

        - name: Create slurm.conf
          copy:
            src: "{{ _slurm_conf_tmpfile.path }}"
            dest: /etc/slurm/slurm.conf
            owner: root
            group: root
            mode: 0644
          when: openhpc_enable.control | default(false) or not openhpc_slurm_configless
          notify:
            - Restart slurmctld service
          register: ohpc_slurm_conf
          # NB uses restart rather than reload as number of nodes might have changed

        - name: Create gres.conf
          template:
            src: "{{ openhpc_gres_template }}"
            dest: /etc/slurm/gres.conf
            mode: "0600"
            owner: slurm
            group: slurm
          when: openhpc_enable.control | default(false) or not openhpc_slurm_configless
          notify:
            - Restart slurmctld service
          register: ohpc_gres_conf
          # NB uses restart rather than reload as this is needed in some cases

        - name: Template cgroup.conf
          # appears to be required even with NO cgroup plugins: https://slurm.schedmd.com/cgroups.html#cgroup_design
          template:
            src: cgroup.conf.j2
            dest: /etc/slurm/cgroup.conf
            mode: "0644" # perms/ownership based off src from ohpc package
            owner: root
            group: root
          when: openhpc_enable.control | default(false) or not openhpc_slurm_configless

        - name: Remove local tempfile for slurm.conf templating
          ansible.builtin.file:
            path: "{{ _slurm_conf_tmpfile.path }}"
            state: absent
          when: _slurm_conf_tmpfile.path is defined
          delegate_to: localhost
          changed_when: false # so molecule doesn't fail
          become: no

        - name: Notify handler for slurmd restart
          debug:
            msg: "notifying handlers" # meta: noop doesn't support 'when'
          changed_when: true
          when:
            - openhpc_slurm_control_host in ansible_play_hosts
            - hostvars[openhpc_slurm_control_host].ohpc_slurm_conf.changed or hostvars[openhpc_slurm_control_host].ohpc_gres_conf.changed # noqa no-handler
          notify:
            - Restart slurmd service

        - name: Set slurmctld location for configless operation
          lineinfile:
            path: /etc/sysconfig/slurmd
            line: "SLURMD_OPTIONS='--conf-server {{ openhpc_slurm_control_host_address | default(openhpc_slurm_control_host) }}'"
            regexp: "^SLURMD_OPTIONS="
            create: yes
            owner: root
            group: root
            mode: 0644
          when:
            - openhpc_enable.batch | default(false)
            - openhpc_slurm_configless
          notify:
            - Restart slurmd service
          # Reloading is sufficent, but using a single handler means no bounce. Realistically this won't regularly change on a running slurmd so restarting is ok.

        # Munge state could be unchanged but the service is not running.
        # Handle that here.
        - name: Configure Munge service
          service:
            name: munge
            enabled: "{{ openhpc_slurm_service_enabled | bool }}"
            state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"

        - name: Flush handler
          meta: flush_handlers # as then subsequent "ensure" is a no-op if slurm services bounced

        - name: Ensure slurmdbd state
          service:
            name: slurmdbd
            enabled: "{{ openhpc_slurm_service_enabled | bool }}"
            state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
          when: openhpc_enable.database | default(false) | bool

        # - name: Ensure slurmctld state
        #   service:
        #     name: slurmctld
        #     enabled: "{{ openhpc_slurm_service_enabled | bool }}"
        #     state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
        #   when: openhpc_enable.control | default(false) | bool

        - name: Ensure slurmd state
          service:
            name: slurmd
            enabled: "{{ openhpc_slurm_service_enabled | bool }}"
            state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
          when: openhpc_enable.batch | default(false) | bool